mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf$:
  loader: llama.cpp
  cpu: false
  threads: 0
  threads_batch: 0
  n_batch: 512
  no_mmap: false
  mlock: false
  no_mul_mat_q: false
  n_gpu_layers: 20
  tensor_split: ''
  n_ctx: 32768
  compress_pos_emb: 1
  alpha_value: 1
  rope_freq_base: 1000000
  numa: false
  no_offload_kqv: false
  row_split: false
  tensorcores: true
  streaming_llm: false
  attention_sink_size: 5
